{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c383fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import csv\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d53cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the path and launching chromedriver\n",
    "PATH = r'chromedriver.exe'\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca72f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_links = [] #List to store the links of the restaurants\n",
    "page=1\n",
    "x=1\n",
    "while page<40:   #Number of pages during scraping was around 40 so thats why we used this as a limit\n",
    "    try: #Try and except in order to get the links of a page\n",
    "        name=driver.find_element_by_xpath(f\"(//*[contains(@class, 'bHGqj Cj b')])[{x}]\") #Use appropriate xpath\n",
    "        a_links.append(name.get_attribute(\"href\"))\n",
    "        #increment x by 1\n",
    "        x+=1\n",
    "    except Exception as e:\n",
    "        break    \n",
    "    if(x%31==0): #Click the button for the next set of restaurants\n",
    "        if page == 1:\n",
    "            python_button =driver.find_element_by_xpath('//*[@id=\"EATERY_LIST_CONTENTS\"]/div[2]/div/a[1]')\n",
    "            python_button.click()\n",
    "            time.sleep(2) #Wait 2sec to load\n",
    "            page+=1\n",
    "        else:\n",
    "            python_button =driver.find_element_by_xpath('//*[@id=\"EATERY_LIST_CONTENTS\"]/div[2]/div/a[2]')\n",
    "            python_button.click()\n",
    "            time.sleep(2)\n",
    "            page+=1 \n",
    "        x=1\n",
    "pd.DataFrame(a_links).to_csv(\"Restaurant_Links.csv\") #Save to csv all the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "related-viewer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      https://www.tripadvisor.com/Restaurant_Review-...\n",
       "1      https://www.tripadvisor.com/Restaurant_Review-...\n",
       "2      https://www.tripadvisor.com/Restaurant_Review-...\n",
       "3      https://www.tripadvisor.com/Restaurant_Review-...\n",
       "4      https://www.tripadvisor.com/Restaurant_Review-...\n",
       "                             ...                        \n",
       "259    https://www.tripadvisor.com/Restaurant_Review-...\n",
       "260    https://www.tripadvisor.com/Restaurant_Review-...\n",
       "261    https://www.tripadvisor.com/Restaurant_Review-...\n",
       "262    https://www.tripadvisor.com/Restaurant_Review-...\n",
       "263    https://www.tripadvisor.com/Restaurant_Review-...\n",
       "Name: 1, Length: 264, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the previous csv and drop duplicates. Duplicates are a result of sponsored restaurants\n",
    "a_links = pd.read_csv('LINKS_FILE_CSV',header=None) #Here we open the links file\n",
    "a_links = a_links[1]\n",
    "a_links.drop_duplicates(inplace = True)\n",
    "a_links.reset_index(drop = True, inplace = True)\n",
    "a_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "republican-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "# Prepare CSV file\n",
    "csvFile = open(\"reviews4.csv\", \"w\", newline='', encoding=\"utf-8\") #Open the file where the reviews are going to be saved\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['Business_Name','Score','Username','Rating_date','Date_of_Visit','Title','Review','Review_Distribution','Age_Gender_Location'])\n",
    "data = [] #Various lists that are going to help for saving the different data that we want to acquire\n",
    "review_distribution = []\n",
    "user_inf = [] \n",
    "my_xpath = '//div[@class=\"info_text pointer_cursor\"]'\n",
    "x_path_x = '//*[@id=\"BODY_BLOCK_JQUERY_REFLOW\"]/span/div[4]'\n",
    "\n",
    "for i in range(len(a_links)):\n",
    "    URL = a_links[i] #Start going through the links\n",
    "    driver.get(URL)\n",
    "    business_name = driver.title.split(',')[0] #Split the link in order to add a variable that will change the pages later\n",
    "    x=1\n",
    "    try: #Use appropriate xpaths\n",
    "        number_of_reviews=driver.find_element_by_xpath('//*[@id=\"taplc_detail_filters_rr_resp_0\"]/div/div[1]/div/div[2]/div[4]/div/div[2]/div[1]/div[2]/label/span[2]').text\n",
    "        number_of_reviews = re.search(r\"\\d+\", number_of_reviews)\n",
    "        num=int(number_of_reviews.group())\n",
    "        split_link=URL.split('Reviews') \n",
    "        while x<=num: #Same as previously we scrape the pages of the reviews of each restaurant\n",
    "            if x==1:\n",
    "                driver.get(split_link[0]+'Reviews-or'+split_link[1]) #Here we add the variable that was mentioned \n",
    "                x=15\n",
    "            else:\n",
    "                try:\n",
    "                    \n",
    "                    driver.get(split_link[0]+'Reviews-or'+f\"{x}-\"+split_link[1])#Here we add the variable that was mentioned \n",
    "                    x=x+15\n",
    "                except Exception as e:\n",
    "                    break\n",
    "            # Find and click the More link (to load all reviews)\n",
    "            try:\n",
    "                driver.find_element_by_xpath(\"//span[@class='taLnk ulBlueLinks']\").click()\n",
    "                time.sleep(4) # Wait for reviews to load\n",
    "            except:\n",
    "                pass\n",
    "            elements = driver.find_elements_by_css_selector(\".info_text.pointer_cursor\")\n",
    "            elements2 = driver.find_elements_by_css_selector(\".prw_rup.prw_reviews_stay_date_hsx\")\n",
    "            reviews = driver.find_elements_by_xpath(\"//div[@class='ui_column is-9']\")\n",
    "            num_page_items = min(len(reviews), 15)\n",
    "            users_info = driver.find_elements_by_xpath(my_xpath)\n",
    "            # Loop through the reviews found\n",
    "            for i, user in enumerate(users_info):\n",
    "                # get the score, date, title and review, date of visit, etc\n",
    "                score_class = reviews[i].find_element_by_xpath(\".//span[contains(@class, 'ui_bubble_rating bubble_')]\").get_attribute(\"class\")\n",
    "                score = score_class.split(\"_\")[3]\n",
    "                rating_date = reviews[i].find_element_by_xpath(\".//span[@class='ratingDate']\").get_attribute(\"title\")\n",
    "                date_of_visit = elements2[i].text\n",
    "                title = reviews[i].find_element_by_xpath(\".//span[@class='noQuotes']\").text\n",
    "                review = reviews[i].find_element_by_xpath(\".//p[@class='partial_entry']\").text.replace(\"\\n\", \"\")\n",
    "                username = elements[i].find_element_by_tag_name(\"div\").text\n",
    "\n",
    "                user.click() #here it is clicking the user in order for the pop up window to show\n",
    "                inf = driver.find_elements_by_xpath('//*[@class=\"countsReviewEnhancementsItem\"]')\n",
    "                time.sleep(1) #Use time sleeps to load the data \n",
    "                data.append(inf)\n",
    "                try:\n",
    "                    member_info = driver.find_element_by_xpath('//*[@class=\"memberdescriptionReviewEnhancements\"]')\n",
    "                    time.sleep(1)\n",
    "                    b = member_info.text\n",
    "                    \n",
    "                except:\n",
    "                    b = None\n",
    "                    pass\n",
    "                try:\n",
    "                    review_dist = driver.find_element_by_xpath('//*[@class=\"wrap container histogramReviewEnhancements\"]')\n",
    "                    time.sleep(1)\n",
    "                    a = review_dist.text\n",
    "                   \n",
    "                except:\n",
    "                    a = None\n",
    "                    pass\n",
    "                time.sleep(1)\n",
    "                users_info = driver.find_element_by_xpath(x_path_x).click() #Saving data\n",
    "                'Business_Name','Score','Username','Rating_date','Date_of_Visit','Title','Review','Review_Distribution','Age_Gender_Location'\n",
    "                csvWriter.writerow(( business_name, score, username, rating_date, date_of_visit, title, review,a,b))# Close CSV file and browser\n",
    "                try:\n",
    "                    users_info[i+1].click() #Moving on\n",
    "                except:\n",
    "                    pass\n",
    "    except:\n",
    "        pass\n",
    "csvFile.close() #Close the csv file\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
